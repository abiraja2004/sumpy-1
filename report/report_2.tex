%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\renewcommand\vec[1]{\overrightarrow{#1}}
\newcommand\cev[1]{\overleftarrow{#1}}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage{mathtools}
\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage{graphicx}
\usepackage{float}

\graphicspath{ {images/} }

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{ $\bullet$ Human Language Technologies - Academic Year 2017/2018 $\bullet$ } % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{TextRank: evaluation \& web-app \\
\large Project Report for Human Language Technologies course } % Article title
\author{%s
	\textsc{Alberto Testoni }\\ [1ex]%\thanks{A thank you or further information} \\[1ex] % Your name
\normalsize CIMeC, University of Trento \\ % Your institution
\normalsize \href{mailto:teston93@gmail.com}{teston93@gmail.com} % Your email address
\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
\textsc{Roberto Dess\'{i} } \\[1ex] % Second author's name
\normalsize  CIMeC, University of Trento\\ % Second author's institution
\normalsize \href{mailto:roberto.dessi11@gmail.com}{roberto.dessi11@gmail.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%

%\begin{abstract}
%\noindent 
%In the report we want to fvs
%\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Motivation}
In this project we'll focus on two classical tasks in Natural Language processing: keyword extraction and text summarization. \textit{Extracting keywords} is one of the most important tasks when dealing with texts. Readers benefit from keywords because they can judge more quickly whether the text is worth reading; website creators can group similar content by its topics; keyword extraction allows programmers to reduce the dimensionality of text to the most important features. Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may provide a concise summary for a given document. Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.\\
The second task consists of \textit{automatic summarization}. In the Internet-era there is a great need to reduce much of this text data to shorter, focused summaries that capture the salient details, both so we can navigate it more effectively as well as check whether the larger documents contain the information that we are looking for. The main idea of summarization is to find a subset of data which contains the "information" of the entire set. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences. There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. In this project, we'll focus on extractive summarization. 

\section{Literature Review}
The field of automated summarization has drawn attention since the late 50's [5]. The mainstream research in this field emphasizes extractive approaches to summarization using statistical methods [6]. Several statistical models have been developed based on training corpora to combine different heuristics using keywords, position and length of sentences, word frequency or titles. Other methods are based in the representation of the text as a graph. The graph-based ranking approaches consider the intrinsic structure of the texts instead of treating texts as simple aggregations of terms. Thus it is able to capture and express richer information in determining important concepts [19].\\
Graph-based ranking algorithms (like the Google's PageRank algorithm by Brin and Page [3]) provide a Web page ranking mechanism that relies on the collective knowledge of Web architecture rather than individual content analysis of pages. The question the authors of [1] address is whether this line of thinking can be applied even to lexical or semantic graphs extracted from natural language documents. To enable the application of graph-based ranking algorithms to natural language texts, it is necessary to build a graph that represents the text, and interconnects words or other text entities with meaningful relations; text units of various sizes and characteristics can be added as vertices in the graph but it is the application that dictates the type of relations that are used to draw connections between any two such vertices. In [1], the authors introduce a graph-based ranking model for text processing named Textrank; in particular, they propose and evaluate two unsupervised methods for keyword and sentence extraction (\textit{extractive summarization}). This graph-based ranking model can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking and selection decisions; for example, in [4] the TextRank algorithm is applied in the task of \textit{word sense disambiguation}. \\
TextRank determines the relation of similarity between two sentences based on the content that both share. This overlap is calculated simply as the number of common lexical tokens between them, divided by the length of each to avoid promoting long sentences. In [2] the authors describe different modifications over the original TextRank algorithm. These ideas are based in changing the way in which distances between sentences are computed to weight the edges of the graph used for TextRank. This approach is able to achieve, through the best configuration, an improvement in performance of 2.92\% over the original TextRank setup. Being TextRank one the most widely used method for unsupervised extractive summarization [1] it seems to be a satisfactory contribution.

\section{Project Proposal}


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------


\bibliographystyle{unsrt}
\begin{thebibliography}{99}

\bibitem{c1} Mihalcea, Rada  and  Tarau, Paul, TextRank: Bringing Order into Texts, Proceedings of EMNLP 2004.
\bibitem{c2} Federico Barrios, Federico L{\'{o}}pez, Luis Argerich, Rosa Wachenchauzer, Variations of the Similarity Function of TextRank for Automated Summarization, CoRR 2016
\bibitem{c3} S. Brin and L. Page. 1998. The anatomy of a large-scale hyper- textual Web search engine. Computer Networks and ISDN Systems, 30(1-7).
\bibitem{c4} R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic networks, with application to word sense disambiguation. In Proceedings of the 20st International Conference on Compu- tational Linguistics (COLING 2004), Geneva, Switzerland.
\bibitem{c5} Luhn, H.P.: The automatic creation of literature abstracts. IBM J. Res. Dev. 2(2), 159-165 (Apr 1958), http://dx.doi.org/10.1147/rd.22.0159
\bibitem{c6} Das, D., Martins, A.F.T.: A survey on automatic text summarization. Tech. rep., Carnegie Mellon University, Language Technologies Institute (2007)

\end{thebibliography}



%----------------------------------------------------------------------------------------

\end{document}
